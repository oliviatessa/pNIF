{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDoIbsKzURJq",
    "outputId": "797ab1d3-7edb-4a3c-ccd7-ae8de9ee97cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/pswpswpsw/nif.git\n",
      "  Cloning https://github.com/pswpswpsw/nif.git to /tmp/pip-req-build-o0er2qy4\n",
      "  Running command git clone -q https://github.com/pswpswpsw/nif.git /tmp/pip-req-build-o0er2qy4\n",
      "  Resolved https://github.com/pswpswpsw/nif.git to commit fd334c1319927041f469899a029ee86a5111ad73\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
      "  Downloading pyparsing-3.0.8-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 6.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting python-dateutil>=2.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[K     |████████████████████████████████| 247 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: NIF\n",
      "  Building wheel for NIF (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NIF: filename=NIF-1.0.0-py3-none-any.whl size=38056 sha256=389bcf6429a2b54de921fc22fe1243d83e3dded9eb0802353d59e8b116606413\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rvk_cd5f/wheels/d5/0b/49/f6a38efe256c8e36a9e0c57d0ca03d6c0111ca7d0266e57a0a\n",
      "Successfully built NIF\n",
      "Installing collected packages: six, python-dateutil, pyparsing, pillow, numpy, kiwisolver, cycler, matplotlib, NIF\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 7.2.0\n",
      "    Uninstalling Pillow-7.2.0:\n",
      "      Successfully uninstalled Pillow-7.2.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.2.0\n",
      "    Uninstalling kiwisolver-1.2.0:\n",
      "      Successfully uninstalled kiwisolver-1.2.0\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.10.0\n",
      "    Uninstalling cycler-0.10.0:\n",
      "      Successfully uninstalled cycler-0.10.0\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.2.2\n",
      "    Uninstalling matplotlib-3.2.2:\n",
      "      Successfully uninstalled matplotlib-3.2.2\n",
      "  Attempting uninstall: NIF\n",
      "    Found existing installation: NIF 1.0.0\n",
      "    Uninstalling NIF-1.0.0:\n",
      "      Successfully uninstalled NIF-1.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfrecorder 2.0 requires numpy<1.19.0, but you have numpy 1.19.5 which is incompatible.\n",
      "tfrecorder 2.0 requires tensorflow==2.3.1, but you have tensorflow 2.6.2 which is incompatible.\n",
      "tensorflow 2.6.2 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow-transform 0.26.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,<2.4,>=1.15.2, but you have tensorflow 2.6.2 which is incompatible.\n",
      "httplib2 0.19.1 requires pyparsing<3,>=2.4.2, but you have pyparsing 3.0.8 which is incompatible.\u001b[0m\n",
      "Successfully installed NIF-1.0.0 cycler-0.11.0 kiwisolver-1.3.1 matplotlib-3.3.4 numpy-1.19.5 pillow-8.4.0 pyparsing-3.0.8 python-dateutil-2.8.2 six-1.16.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/intel/intelpython3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall git+https://github.com/pswpswpsw/nif.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to training on large dataset (>5GB) where data cannot fit in memory?\n",
    "\n",
    "### Note \n",
    "\n",
    "- GPU memory is precious, don't waste it on dataset\n",
    "- It can be super interesting to train NIF on HUGE dataset!\n",
    "- In this Google Colab, I will show a demonstration using [tensorflow record](https://www.tensorflow.org/tutorials/load_data/tfrecord),\n",
    "which is a simple format for storing a sequence of binary records.\n",
    "- First you will learn how to obtain `tfrecord` file from `npz`, then you will\n",
    "learn how to perform training across different `tfrecord` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prepare a \"large\" numpy `npz` to play with\n",
    "\n",
    "- this \"ficitious\" dataset has 7 dimensions, \n",
    "$$(t,x,y,z,u,v,w)$$\n",
    "where $u,v,w$ are three components of velocity field. \n",
    "- Thus, we have **4 features, and 3 targets, no weight**. We will need this information to create the dataset \n",
    "\n",
    "- let's call it `Big_npz_file.npz`, it has 54 MB with $10^6$ points, each point contains 7 real number values\n",
    "- it has a key argument `data`, which stores the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.uniform(0,1,(1000000,7))\n",
    "np.savez('Big_npz_file.npz', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate `tfrecord` files from a big a `npz` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to bring the `TFRDataset` class under `nif.data.tfr_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Vx__GYjvVbcY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nif.data.tfr_dataset import TFRDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, create an instance, a \"file handler\", call it `fh`, given the `n_features = 3` and `n_target = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = TFRDataset(n_feature=3, n_target=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, you need to decide \n",
    "1. How many points to store in one single **tfrecord** file ----> `num_pts_per_file`\n",
    "    - it doesn't have to be perfectly dividing the total number of points, we will make the last one smaller if it doesn't have a perfect divide.\n",
    "    - Tensorflow official document [suggest](https://www.tensorflow.org/tutorials/load_data/tfrecord) around 100 MB.\n",
    "    \n",
    "2. What's the `npz` filepath? ----> `npz_path`\n",
    "3. What's the key to access the 2-D matrix pointwise data? ----> `npz_key`\n",
    "4. Where do you want to put the generated tfrecord file? ----> `tfr_path`\n",
    "5. What's the prefix you want to put in the filename for the tfrecord file? \n",
    "    - they will be something like `prefix_0.npz`, ..., `prefix_10.npz`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  If you have answers to the above in your mind, now you can just call \n",
    "### `fh.create_from_npz(num_pts_per_file, npz_path, npz_key, write_tfr_path, prefix)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh.create_from_npz(num_pts_per_file=5e6, npz_path='Big_npz_file.npz',\n",
    "                   npz_key='data', tfr_path='TFR_dir',\n",
    "                   prefix=\"case1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, you can see the **tfrecord** files that you generated! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls TFR_dir -lrth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How to train a Keras Model on the tfrecord files that generated above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a toy Model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "x = tf.keras.Input(4,)\n",
    "l1 = tf.keras.layers.Dense(3, activation='tanh')\n",
    "model = tf.keras.Model([x], [l1(x)])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a **meta dataset** or maybe you can call it **hyper dataset**. We will do something called **sub-dataset-batching** (I called it). \n",
    "\n",
    "- First you need to determine how many epoch you want to run ----> `epoch`\n",
    "- Then we call the method `.get_tfr_meta_dataset(path, epoch)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "meta_dataset = fh.get_tfr_meta_dataset(tfr_path='TFR_dir', epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we do sub-dataset batching with `meta_dataset`\n",
    "\n",
    "- we obtain `batch_file`, which are Tensors basically, from meta_dataset\n",
    "- we create a `sub` dataset from `batch_file`\n",
    "\n",
    "- you can add your favourite callbacks if you want\n",
    "- the default `epoch` in `model.fit` is 1, which makes sense in this context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "batch_size = 128\n",
    "\n",
    "for batch_file in meta_dataset:\n",
    "    batch_dataset = fh.gen_dataset_from_batch_file(batch_file, batch_size)\n",
    "    model.fit(batch_dataset, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cautious! \n",
    "- it is not perfect since we are `.fit` for each `tfrecord` and this will make epoch information lost after each recall of `.fit`\n",
    "- we can hack it by using a `callbacks` that explicitly write loss to the disk.. in an append way.\n",
    "- **again what is presented here is not perfect, I am open for your solutions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "- **Why are you choosing to chop tfrecord like this?** It seems to me it is not what the official document has suggested, the syntax here looks very not software-optimized. \n",
    "\n",
    "### **Answer:** \n",
    "\n",
    "- Because the standard way of using `tfrecord` will take each data points, i.e., $(t,x,y,z,u,v,w)$ as an `tf.train.Example`. When you have 10 of millions, hundreds of millsion data points, **generating such dataset (not using this dataset for training)** has been reported to have performance issues if your algorithm contains any native Python Loop, which is not avoidable at the current stage. \n",
    "\n",
    "- In `nif.data.tfr_dataset.TFRDataset`, we subsampling npz file in a sequential order and take data along each dimension (e.g., (t_0,...,t_M) ) as an `tf.train.Example`, instead of treating each single point `(t,x,y,z,u,v,w)` as an example. So we have 7 `Example` in this demo. \n",
    "\n",
    "- In this way, performance issue is greatly reduced. I believe the reason is that we leave the inner looping over all datapoints to the low-level\n",
    "\n",
    "- So far, I can generate tfrecord from tens of Gigabyte data within 30 mins to 1 hour. While if I tried to make each training example as data point, it may take \n",
    "\n",
    "### Let me know <shawnpan@uw.edu>  if you have better solution for this, i.e., without using the meta-dataset to do sub-dataset-batching"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4-get-gradients-by-wrapping-model-with-layer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 with Fil",
   "language": "python",
   "name": "filprofile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
